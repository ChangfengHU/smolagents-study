import os
import sys
from pathlib import Path

# Make local "src" importable without installing the package (useful for quick local runs / PyCharm)
#https://platform.openai.com/docs/pricing
PROJECT_ROOT = Path(__file__).resolve().parents[1]
SRC_DIR = PROJECT_ROOT / "src"
if str(SRC_DIR) not in sys.path:
    sys.path.insert(0, str(SRC_DIR))

from dotenv import load_dotenv  # type: ignore
from smolagents import CodeAgent, OpenAIServerModel, WebSearchTool, ToolCallingAgent
from smolagents.models import ChatMessageStreamDelta, LiteLLMModel

PROMPT = "è¯·ä¸ºæˆ‘åˆ¶å®šä¸€ä¸ªè¯¦ç»†çš„3å¤©æ­å·æ—…è¡Œè®¡åˆ’ã€‚è¦æ±‚ï¼š1. æ¯å¤©å®‰æ’2ä¸ªä¸»è¦æ™¯ç‚¹ï¼›2. ä¸ºæ¯ä¸ªæ™¯ç‚¹æä¾›é€‰æ‹©ç†ç”±ï¼›3. åŒ…å«å¤©æ°”æŸ¥è¯¢å’Œç›¸åº”çš„å»ºè®®ï¼›4. æ¨èæ¯ä¸ªåŒºåŸŸçš„ç‰¹è‰²é¤å…ï¼›5. æä¾›äº¤é€šå»ºè®®ã€‚è¯·ä½¿ç”¨å·¥å…·æŸ¥è¯¢ç›¸å…³ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆç»“æ„åŒ–çš„è¡Œç¨‹è¡¨ã€‚"


def main() -> None:
    # Load .env if present
    load_dotenv()
     # gpt-5-nano	$0.05
    #  gpt-4.1-nano	$0.10
    #gpt-4o-mini	$0.15
    api_key = os.getenv("OPENAI_API_KEY")
    api_base = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
    model_id = os.getenv("OPENAI_MODEL_ID", "gpt-4.1-nano")
    # model_id = os.getenv("OPENAI_MODEL_ID", "gpt-3.5-turbo")
    # model_id = os.getenv("OPENAI_MODEL_ID", "gpt-4o-mini")
    # model_id = os.getenv("OPENAI_MODEL_ID", "gpt-5-nano-2025-08-07")

    # If OPENAI_API_KEY is missing, do a friendly dry-run instead of crashing
    if not api_key:
        print("[dry-run] æœªæ£€æµ‹åˆ°ç¯å¢ƒå˜é‡ OPENAI_API_KEYã€‚è¯·åœ¨ PyCharm çš„ Run/Debug é…ç½®é‡Œæ·»åŠ ï¼š")
        print("  OPENAI_API_KEY=<ä½ çš„OpenAIå¯†é’¥>")
        print("å¯é€‰ï¼šOPENAI_API_BASE=https://api.openai.com/v1  OPENAI_MODEL_ID=gpt-4o-mini")
        print("ç„¶åå†æ¬¡è¿è¡Œæœ¬è„šæœ¬å³å¯çœŸæ­£è°ƒç”¨æ¨¡å‹ã€‚\n")
        print("æœ¬è„šæœ¬å°†ä½¿ç”¨ WebSearchToolï¼ˆDuckDuckGo æŠ“å–ï¼‰ä¸ OpenAIServerModel ç»„åˆã€‚")
        print(f"PROMPT: {PROMPT}")
        return

    # model = LiteLLMModel(
    #     model_id="ollama/gemma3",
    #     api_base="https://ollama.vyibc.com",
    #     api_key="ollama",
    # )

    model = LiteLLMModel(
        model_id="ollama/gemma3",
        # model_id="ollama/qwen3:latest",

        api_base="http://127.0.0.1:11434",
        api_key="ollama",
    )
    # ä½¿ç”¨æ— éœ€é¢å¤–ä¾èµ–çš„ WebSearchToolï¼ˆå†…éƒ¨ç”¨ requestsï¼‰
    tools = [WebSearchTool(max_results=10, engine="duckduckgo")]

    agent = ToolCallingAgent(
        tools=tools,
        model=model,
        verbosity_level=1,
        stream_outputs=True,
        planning_interval=3,
        name="hangzhou_trip_agent",
        description="ä¸“ä¸šçš„æ­å·æ—…è¡Œè§„åˆ’åŠ©æ‰‹ï¼Œèƒ½å¤Ÿåˆ¶å®šè¯¦ç»†çš„3å¤©è¡Œç¨‹å®‰æ’ï¼ŒåŒ…æ‹¬æ™¯ç‚¹æ¨èã€ç”¨é¤å»ºè®®å’Œäº¤é€šæŒ‡å—ã€‚",
        instructions="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…è¡Œè§„åˆ’åŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡è¿›è¡Œæ€è€ƒå’Œå›ç­”ã€‚

åœ¨è§£å†³ä»»åŠ¡æ—¶ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š
1. åœ¨"æ€è€ƒï¼š"éƒ¨åˆ†ï¼Œç”¨ä¸­æ–‡è§£é‡Šä½ çš„æ¨ç†è¿‡ç¨‹å’Œè¦ä½¿ç”¨çš„å·¥å…·
2. åœ¨ä»£ç éƒ¨åˆ†ç¼–å†™Pythonä»£ç æ¥æ‰§è¡Œä»»åŠ¡
3. åœ¨"è§‚å¯Ÿï¼š"éƒ¨åˆ†ï¼Œç”¨ä¸­æ–‡æ€»ç»“ä»£ç æ‰§è¡Œçš„ç»“æœ
4. æœ€åç”¨ä¸­æ–‡æä¾›æœ€ç»ˆç­”æ¡ˆ

è¯·ç¡®ä¿æ‰€æœ‰çš„æ€è€ƒè¿‡ç¨‹ã€è§‚å¯Ÿç»“æœå’Œæœ€ç»ˆç­”æ¡ˆéƒ½ä½¿ç”¨ä¸­æ–‡è¡¨è¾¾ã€‚""",
    )

    # result = agent.run(PROMPT)
    for step in agent.run(PROMPT, stream=True):
        if isinstance(step, ChatMessageStreamDelta):
            continue  # è·³è¿‡ ChatMessageStreamDelta ç±»å‹çš„æ­¥éª¤
        print(f"æ”¶åˆ°æ­¥éª¤: {step}")  # ğŸ”¥ æœ€ç»ˆæ¥æ”¶åˆ°æ‰€æœ‰event
    print("\n=== æœ€ç»ˆç»“æœ ===\n")
    # print(result)


if __name__ == "__main__":
    main()
